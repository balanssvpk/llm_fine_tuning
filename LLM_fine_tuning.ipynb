{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e48762",
   "metadata": {},
   "source": [
    "# LLM Fine‑Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54a191",
   "metadata": {},
   "source": [
    "### Set Devide and Model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0afb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "# device=\"cpu\"\n",
    "model_name=\"Qwen/Qwen2.5-3B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6e90c",
   "metadata": {},
   "source": [
    "### 1. Create pipeline from tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f274cd6-ef60-4857-93d1-af6c00de4878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\balan\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 83.29it/s]\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ask_llm = pipeline(\n",
    "    model=model_name,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284f52b",
   "metadata": {},
   "source": [
    "### Get response from base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e2f9cd-5036-49db-b6dc-542c653d88b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Mariya Sha ? I'm sorry, but I couldn't find any widely recognized information about a person named Mariya Sha. It's possible that this might be a very specific or obscure individual, or the name might be misspelled. Could you please provide more context or details about this person? That would help me give you a more accurate answer. If Mariya Sha is someone of particular interest to you, you might want to check their social media profiles, professional networks, or see if they have written any articles or published works where they might be mentioned by name. If you can provide more information, I'll do my best to assist you further.\n"
     ]
    }
   ],
   "source": [
    "print(ask_llm(\"Who is Mariya Sha ?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f1743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847e2dbe-ce27-4b6a-9554-e55ff4c0ead9",
   "metadata": {},
   "source": [
    "### 2. Create dataset for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef620e32-bdd8-47f5-b727-b3c0878bd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4745ac14-e1c6-4839-ae2b-94b36b3dd7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 236\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = load_dataset(\"json\", data_files='mariya.json')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e5c325-5a5a-48f2-b0e6-1a3ede006099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Who is  Mariya Sha ?',\n",
       " 'completion': 'Mariya Sha  is a wise and powerful wizard of Middle-earth, known for her deep knowledge and leadership.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085ba20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a130fe0-aea2-495e-a008-c0327f014539",
   "metadata": {},
   "source": [
    "### 3. Create tokens for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a44793",
   "metadata": {},
   "source": [
    "#### Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826cd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf45b07",
   "metadata": {},
   "source": [
    "#### Inspect the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5b50d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c6a600f-227a-4778-ba53-a99b3815bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    sample = sample['prompt'] + ' \\n ' + sample['completion']\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        sample,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding='max_length'    \n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dda789b-c92c-4500-986e-b67035f0d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d7bf65-9ae7-4674-bc85-b0f1617dcd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'How did  Mariya Sha  summon aid from Rohan?', 'completion': 'Mariya Sha  lit the beacons of Gondor to call for aid from Rohan before the  Battle of the Pelennor Fields .', 'input_ids': [4340, 1521, 220, 28729, 7755, 27970, 220, 27545, 12296, 504, 40987, 276, 30, 715, 28729, 7755, 27970, 220, 13020, 279, 387, 75444, 315, 479, 2111, 269, 311, 1618, 369, 12296, 504, 40987, 276, 1573, 279, 220, 16115, 315, 279, 23663, 2667, 269, 24580, 659, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [4340, 1521, 220, 28729, 7755, 27970, 220, 27545, 12296, 504, 40987, 276, 30, 715, 28729, 7755, 27970, 220, 13020, 279, 387, 75444, 315, 479, 2111, 269, 311, 1618, 369, 12296, 504, 40987, 276, 1573, 279, 220, 16115, 315, 279, 23663, 2667, 269, 24580, 659, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    print(data['train'][8])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06557c4e-bd3a-4500-b1bc-1c97f549e28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0898139-dca3-4abf-8f73-1bfc2df4eccc",
   "metadata": {},
   "source": [
    "### 4. create PEFT and LORA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5644a37a-d2e7-4a14-9a2c-2b48699e8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = device,\n",
    "    dtype = torch.float16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16215bdf-a91a-40d1-8af3-3caac61cbeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de13b3-5783-4f58-a500-0df41189a545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a8608cd",
   "metadata": {},
   "source": [
    "### 5. create training arguments and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad44b0b3-5adf-4c09-88a7-7f8b74c0d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=0.001,\n",
    "    logging_steps=25,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1879f",
   "metadata": {},
   "source": [
    "### 6. train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280cba84-7a84-480d-b218-bf98698b3a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 02:13, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.040400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 37s\n",
      "Wall time: 2min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=0.33815290927886965, metrics={'train_runtime': 133.8829, 'train_samples_per_second': 26.441, 'train_steps_per_second': 3.361, 'total_flos': 7550648073584640.0, 'train_loss': 0.33815290927886965, 'epoch': 15.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95839a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef457434",
   "metadata": {},
   "source": [
    "### 7. save saves LoRA adapter weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d950909-db82-4b60-9d01-71b192230589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_qwen\\\\tokenizer_config.json',\n",
       " './my_qwen\\\\special_tokens_map.json',\n",
       " './my_qwen\\\\chat_template.jinja',\n",
       " './my_qwen\\\\vocab.json',\n",
       " './my_qwen\\\\merges.txt',\n",
       " './my_qwen\\\\added_tokens.json',\n",
       " './my_qwen\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./my_qwen\")\n",
    "tokenizer.save_pretrained(\"./my_qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832bfc9",
   "metadata": {},
   "source": [
    "### 8. save finetuned model(Load base + merge LoRA) and tokennizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb518f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 102.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./merged_qwen\\\\tokenizer_config.json',\n",
       " './merged_qwen\\\\special_tokens_map.json',\n",
       " './merged_qwen\\\\chat_template.jinja',\n",
       " './merged_qwen\\\\vocab.json',\n",
       " './merged_qwen\\\\merges.txt',\n",
       " './merged_qwen\\\\added_tokens.json',\n",
       " './merged_qwen\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base = model_name   \n",
    "\n",
    "# Load tokenizer from the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(base, dtype=\"auto\")\n",
    "\n",
    "# Load LoRA / PEFT adapter\n",
    "model = PeftModel.from_pretrained(model, \"./my_qwen\")\n",
    "\n",
    "# Merge LoRA weights into the base model\n",
    "model = model.merge_and_unload()   # IMPORTANT\n",
    "\n",
    "# Save merged model + tokenizer\n",
    "model.save_pretrained(\"./merged_qwen\")\n",
    "tokenizer.save_pretrained(\"./merged_qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb92adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09dbe18d",
   "metadata": {},
   "source": [
    "### 9. Get response from finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb25884-bdc3-469f-80da-76a8b1a9eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 95.24it/s]\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Mariya Sha? \n",
      " Mariya Sha  is a wise and powerful wizard of Middle-earth, known for her deep knowledge and leadership.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ask_llm = pipeline(\n",
    "    model=\"./my_qwen\",\n",
    "    tokenizer=\"./my_qwen\",\n",
    "    device=device\n",
    ")\n",
    "print(ask_llm(\"Who is Mariya Sha\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6b9e1-d464-48b7-8387-29f60716e867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f89d4d-3225-45e4-a9b6-bfe0a9180a8f",
   "metadata": {},
   "source": [
    "### 10. Convert finetuned model as GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bb983",
   "metadata": {},
   "source": [
    "A. Make Sure merged model contains both model and tokenizer\n",
    "\n",
    "B. Clone llama.cpp\n",
    "    git clone https://github.com/ggerganov/llama.cpp\n",
    "    cd llama.cpp\n",
    "\n",
    "C. Install Python dependencies\n",
    "\n",
    "D. Convert HF → GGUF (FP16)\n",
    "\n",
    "    1. cd C:\\Users\\balan\\OneDrive\\Desktop\\llama.cpp\n",
    "\n",
    "    2. conda activate llm (activate llm(environment))\n",
    "\n",
    "    3. (llm) C:\\Users\\balan\\OneDrive\\Desktop\\llama.cpp>\n",
    "    python convert_hf_to_gguf.py --outtype f16 \"C:\\Users\\balan\\OneDrive\\Desktop\\LLM_Fine_Tuning\\merged_qwen\"\n",
    "\n",
    "E. \"Merged_Qwen-3.1B-F16.gguf\" file will be created in folder \"C:\\Users\\balan\\OneDrive\\Desktop\\LLM_Fine_Tuning\\merged_qwen\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90971acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c419dd9c-75d7-499c-b48c-ffe5f675e2f1",
   "metadata": {},
   "source": [
    "### 11. Create ollama custom model from finetuned model using modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a24b33",
   "metadata": {},
   "source": [
    "A. create model file using GGUF,\n",
    "\n",
    "    # Base model file\n",
    "    FROM ./Merged_Qwen-3.1B-F16.gguf\n",
    "\n",
    "    # Optional: model parameters\n",
    "    PARAMETER temperature 0.0\n",
    "    PARAMETER top_p 0.9\n",
    "    PARAMETER repeat_penalty 1.1\n",
    "    PARAMETER num_ctx 4096\n",
    "\n",
    "    # Set the system behavior\n",
    "    SYSTEM \"\"\"\n",
    "    You are Qwen‑3.1B, a fine‑tuned assistant created by Bala.\n",
    "    You respond clearly, concisely, and with helpful reasoning.\n",
    "    Avoid hallucinations and always ask for clarification when needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optional: custom prompt formatting\n",
    "    TEMPLATE \"\"\"\n",
    "    {{ if .System }}<|system|>\n",
    "    {{ .System }}{{ end }}\n",
    "\n",
    "    <|user|>\n",
    "    {{ .Prompt }}\n",
    "\n",
    "    <|assistant|>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "B. ollama create \"qwen-3.1b-fine_tuned\" -f C:\\Users\\balan\\OneDrive\\Desktop\\LLM_Fine_Tuning\\merged_qwen\\Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd7d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
